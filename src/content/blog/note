```python
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from tqdm import tqdm
import os
```

We need to convert text into vectors as a key functionality. Our goal is to build a chatbot for French documents, so we researched embedding models available on Hugging Face. Using the leaderboard at [this link](https://huggingface.co/spaces/mteb/leaderboard), we selected `bge-m3-custom-fr` for its optimal balance between computational efficiency and performance. Refer to <a href="#image-2">Image 2</a> below, which illustrates the model parameters we considered for this choice.

![RAG Architecture](../../assets/images/RAG/model.png)  
**Image 2: Model Parameter Selection for RAG Workflow**

We then create an instance of our embedding model using the following code:

```python
embed_model = HuggingFaceEmbedding(model_name="manu/bge-m3-custom-fr")
```
Once we have the embedding model, we need a storage database to store the vectors of document chunks that our chatbot will use.

```bash
# Let's create database folder to setup our DB
! mkdir database
```
We then need to create a `docker-compose.yml` file to provision a PostgreSQL database that supports `pgvector`. This support is important because `pgvector` allows us to efficiently handle and query vector data within our database.
```yaml
%%writefile database/docker-compose.yml
version: '3.8'
services:
  RAG_DB:
    image: ankane/pgvector
    container_name: rag_vector_db
    environment:
      POSTGRES_DB: rag_vector_db
      POSTGRES_USER: rag_user
      POSTGRES_PASSWORD: rag_password
    ports:
      - "5433:5432"
    volumes:
      - ./pgdata:/var/lib/postgresql/data
```
We then start our Docker container to provision the database server:

```bash
! cd database ; docker-compose up -d RAG_DB
```
Expected output:

```log
[+] Running 1/0
 ✔ Container rag_vector_db  Running 
 ```

After installing and starting our Docker container that provisions a database server on port 5433, we connect to the database and set it up. We use `pgvector` for efficient manipulation of vector data. The following code connects to the server and creates our database:

```python
import psycopg2

# DB Parameters
db_name = "rag_vector_db"
host = "localhost"
password = "rag_password"
port = "5433"
user = "rag_user"

# Connect and create db
conn = psycopg2.connect(
    dbname="postgres",
    host=host,
    password=password,
    port=port,
    user=user,
)
conn.autocommit = True
with conn.cursor() as c:
    c.execute(f"DROP DATABASE IF EXISTS {db_name}")
    c.execute(f"CREATE DATABASE {db_name}")
```
Next, we set up **PGVectorStore**, which provides functionality for writing and querying vector data in PostgreSQL:

```python
from llama_index.vector_stores.postgres import PGVectorStore

vector_store = PGVectorStore.from_params(
    database=db_name,
    host=host,
    password=password,
    port=port,
    user=user,
    table_name="rag_paper_fr",
    embed_dim=1024,  # (384) openai embedding dimension
)
```

1. Load Data: We have folder named `documents` which contain .pdf documents. This spinet of code extract text from each pdf and store them as list of docements.
```python
from pathlib import Path
from llama_index.readers.file import PyMuPDFReader

# Utils
loader = PyMuPDFReader()
directory_path = Path("./documents")
pdf_files = directory_path.glob("*.pdf")

# Process and rename all PDF files
documents = []
for file_path in pdf_files:
    loaded_docs = loader.load(file_path=str(file_path))
    documents.extend(loaded_docs)
    treated_file_path = file_path.with_name(f"{file_path.stem}.pdf")
    file_path.rename(treated_file_path)
```

2. Create document chuncks: Chunking plays a crucial role in building RAG. Since the documents can be large, it’s necessary to split them into manageable sizes to ensure efficient indexing and retrieval. We must define a chunk_size, which specifies the number of tokens each chunk will contain.

```python
from llama_index.core.node_parser import SentenceSplitter
text_parser = SentenceSplitter(
    chunk_size=1024,
)

text_chunks = []
doc_idxs = [] #<- to save index
for doc_idx, doc in enumerate(documents):
    cur_text_chunks = text_parser.split_text(doc.text)
    text_chunks.extend(cur_text_chunks)
    doc_idxs.extend([doc_idx] * len(cur_text_chunks))
```

3. Let's link each chunck to document sources metadata (Node Chunk)
```python
from llama_index.core.schema import TextNode

nodes = []
for idx, text_chunk in enumerate(text_chunks):
    node = TextNode(
        text=text_chunk,
    )
    src_doc = documents[doc_idxs[idx]]
    node.metadata = src_doc.metadata
    nodes.append(node)
```

4. Generate embeddings for each Node
```python
for node in tqdm(nodes, ncols=100, desc="Generating embedding: "):
    node_embedding = embed_model.get_text_embedding(
        node.get_content(metadata_mode="all")
    )
    node.embedding = node_embedding
```
Generating embedding: 100%|███████████████████████████████████████| 166/166 [01:14<00:00,  2.24it/s]
```pyhton
nodes[0].dict().keys()
```
dict_keys(['id_', 'embedding', 'metadata', 'excluded_embed_metadata_keys', 'excluded_llm_metadata_keys', 'relationships', 'text', 'mimetype', 'start_char_idx', 'end_char_idx', 'text_template', 'metadata_template', 'metadata_seperator', 'class_name'])

```python
nodes[0].dict()['id_']
```
'cf92f9c9-834c-4e2e-896d-1b11dfb1fc0a'

```python
nodes[0].dict()['metadata']
```
{'total_pages': 6,
 'file_path': 'documents/UM6P-Disciplinary_Council.pdf',
 'source': '1'}

We could the the built embedding here: 

```python
nodes[0].dict()['embedding']
```
[-0.030031858012080193,
 -0.038829609751701355,
 -0.0006665196851827204,
 -0.06863262504339218,
 -0.033711548894643784,
 0.020719563588500023,
 0.014222200959920883,
 -0.0631137564778328,
 -0.029535768553614616,
 -0.0323270708322525,
 0.002012217417359352,
 -0.012179107405245304,
 -0.01191095542162656
  ...
 -0.01191095542162656]

```python
len(nodes[0].dict()['embedding'])
```
1024

5. Store embedding vector into PostgresSQL DB
```python
vector_store.add(nodes)
```